defaults:
  - logging: default
  - data: fineweb
  - model: 1B
  - optimizer: adam
  - _self_

model:
  type: diffusion
  diffusion_process: mdlm
  t_eps: 1e-4

training:
  resume: null
  seed: 1
  train_batch_size: 8
  eval_batch_size: 8
  num_train_steps: 2000 #1000_000
  lr_schedule: cosine
  warmup_steps: 10000
  low_discrepancy_sampling: True
  dtype: bf16
  compile_model: True

loss:
  loss_type: mdlm
  loss_scale: 1.0
  reduction: tokenmean
