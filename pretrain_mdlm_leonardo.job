#!/bin/bash
#SBATCH --account=aifac_l01_028
#SBATCH --job-name=mdlm_train_leonardo
#SBATCH --partition=boost_usr_prod
#SBATCH --time=1:00:00
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=1     # Let torchrun handle the GPUs
#SBATCH --cpus-per-task=32
#SBATCH --gpus-per-node=4
#SBATCH --output=slurm_logs/%x-%j.out


# Usage:
#     sbatch pretrain_mdlm_leonardo.job
#

set -e  # Exit immediately on error
# ============================================================================
# SOFTWARE
# ============================================================================
# Activate environment
module purge
module load python/3.11.7
module load nccl/2.22.3-1--gcc--12.2.0-cuda-12.2-spack0.22
export VENV_PATH=".venv"
source $VENV_PATH/bin/activate

echo "Job started at: $(date)"

# ============================================================================
# DISTRIBUTED
# ============================================================================
echo "Allocated nodes:"
scontrol show hostnames $SLURM_JOB_NODELIST

NODE_COUNT=$(scontrol show hostnames $SLURM_JOB_NODELIST | wc -l)
echo "Number of nodes allocated: $NODE_COUNT"

export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT=39591

export NNODES=$SLURM_NNODES
export GPUS_PER_NODE=${SLURM_GPUS_PER_NODE}
TOTAL_GPUS=$((NNODES * GPUS_PER_NODE))

echo "MASTER_ADDR: $MASTER_ADDR"
echo "Running on node: $(hostname)"

DISTRIBUTED_ARGS=(
    --nproc-per-node $SLURM_GPUS_PER_NODE 
    --nnodes $SLURM_NNODES
)

# Check if the nodes establish connection
# export NCCL_DEBUG=INFO

# Offline wandb logging
export WANDB_MODE=offline

# Launch training
srun torchrun "${DISTRIBUTED_ARGS[@]}" \
    --rdzv_id=$SLURM_JOBID \
    --rdzv_backend=c10d \
    --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT \
    --master_addr=$MASTER_ADDR \
    --master_port=$MASTER_PORT \
    gidd/train.py \
    --config-name=mdlm \
    logging.run_name="'1B-mdlm-fineweb-${TOTAL_GPUS}gpu-${SLURM_NNODES}node-lrdn-a100-8mbs'"\