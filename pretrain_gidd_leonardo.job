#!/bin/bash
#SBATCH --job-name=gidd_train_leonardo
#SBATCH --partition=boost_usr_prod
#SBATCH --time=3:00:00
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=1     # Let torchrun handle the GPUs
#SBATCH --cpus-per-task=16
#SBATCH --gpus-per-node=4
#SBATCH --output=slurm_logs/%x-%j.out


# Usage:
#     sbatch pretrain_gidd_leonardo.job
#

set -e  # Exit immediately on error
# ============================================================================
# SOFTWARE
# ============================================================================
# Activate environment
module purge
module load python/3.11.7
module load nccl/2.22.3-1--gcc--12.2.0-cuda-12.2-spack0.22
export VENV_PATH=".venv"
source $VENV_PATH/bin/activate

# ============================================================================
# DISTRIBUTED
# ============================================================================
echo "Allocated nodes:"
scontrol show hostnames $SLURM_JOB_NODELIST

export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT=39591

export NNODES=$SLURM_NNODES
export GPUS_PER_NODE=${SLURM_GPUS_PER_NODE}

echo "MASTER_ADDR: $MASTER_ADDR"
echo "Running on node: $(hostname)"

DISTRIBUTED_ARGS=(
    --nproc-per-node $SLURM_GPUS_PER_NODE 
    --nnodes $SLURM_NNODES
)

export WDIR="/leonardo_work/AIFAC_L01_028/donutu/gidd"


# Offline wandb logging
export WANDB_MODE=offline


# Launch training
srun torchrun "${DISTRIBUTED_ARGS[@]}" \
    --rdzv_id=$SLURM_JOBID \
    --rdzv_backend=c10d \
    --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT \
    --master_addr=$MASTER_ADDR \
    --master_port=$MASTER_PORT \
    gidd/train.py \
    --config-name=gidd \
    logging.run_name="'small-gidd-fineweb-pu=0.0-xgpu-xnode-lrdn'"
